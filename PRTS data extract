# Coding in Python
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
from pprint import pprint

def get_prts_page(url):
    """获取PRTS维基页面（带错误处理和编码设置）"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://prts.wiki/'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"请求失败: {e}")
        return None

def extract_section_data(soup, section_id=None):
    """
    提取指定子目录下的所有数值数据
    :param soup: BeautifulSoup对象
    :param section_id: 可选，直接定位指定子目录的ID
    :return: 字典格式 {子目录标题: 内容数据}
    """
    # 1. 移除干扰元素
    for element in soup.find_all(['script', 'style', 'span.mw-editsection', 'table.navbox']):
        element.decompose()

    # 2. 定位内容区域
    content = soup.find('div', id='mw-content-text')
    if not content:
        return {}

    # 3. 提取所有子目录
    sections = {}
    current_heading = "概述"
    current_content = []

    for element in content.find_all(['p', 'h2', 'h3', 'h4', 'table', 'ul', 'ol']):
        # 处理标题元素
        if element.name in ['h2', 'h3', 'h4']:
            if current_content:
                sections[current_heading] = process_content(current_content)
                current_content = []
            current_heading = element.get_text().strip()
        
        # 处理内容元素
        else:
            current_content.append(element)
    
    if current_content:
        sections[current_heading] = process_content(current_content)
    
    return sections

def process_content(elements):
    """处理单个子目录下的所有内容元素"""
    structured_data = []
    
    for element in elements:
        # 处理表格数据
        if element.name == 'table':
            table_data = parse_table(element)
            if table_data:
                structured_data.append(('table', table_data))
        
        # 处理列表数据
        elif element.name in ['ul', 'ol']:
            list_items = [li.get_text(strip=True) for li in element.find_all('li')]
            structured_data.append(('list', list_items))
        
        # 处理段落文本
        elif element.name == 'p':
            text = clean_text(element.get_text())
            if text:
                structured_data.append(('text', text))
    
    return structured_data

def parse_table(table):
    """解析表格数据为二维字典"""
    rows = []
    for tr in table.find_all('tr'):
        cells = []
        for td in tr.find_all(['th', 'td']):
            # 合并跨行跨列单元格内容
            rowspan = int(td.get('rowspan', 1))
            colspan = int(td.get('colspan', 1))
            cell_text = clean_text(td.get_text())
            cells.extend([cell_text] * colspan)
        if cells:
            rows.append(cells)
    return rows

def clean_text(text):
    """清理文本中的干扰符号"""
    text = re.sub(r'\[\s*编辑\s*\]|\[\d+\]', '', text)
    return re.sub(r'\s+', ' ', text).strip()

def crawl_prts_page(url):
    """执行完整爬取流程"""
    print(f"开始爬取: {url}")
    html = get_prts_page(url)
    if not html:
        return None
    
    soup = BeautifulSoup(html, 'lxml')
    char_name = soup.find('h1', id='firstHeading').get_text()
    print(f"角色名称: {char_name}")
    
    # 提取所有子目录数据
    sections = extract_section_data(soup)
    
    return {
        'name': char_name,
        'url': url,
        'sections': sections
    }

def print_section_data(data):
    """格式化输出所有子目录数据"""
    print(f"\n【{data['name']}】完整数据：")
    for section, content in data['sections'].items():
        print(f"\n=== {section} ===")
        for item_type, item_data in content:
            if item_type == 'table':
                print("\n[表格数据]")
                for row in item_data:
                    print(" | ".join(row))
            elif item_type == 'list':
                print("\n[列表数据]")
                for i, item in enumerate(item_data, 1):
                    print(f"{i}. {item}")
            elif item_type == 'text':
                print(f"\n[文本] {item_data}")

if __name__ == "__main__":
    target_url = "https://prts.wiki/w/%E6%96%B0%E7%BA%A6%E8%83%BD%E5%A4%A9%E4%BD%BF"
    result = crawl_prts_page(target_url)
    
    if result:
        # 打印所有子目录数据
        print_section_data(result)
        
        # 保存到JSON文件（可选）
        import json
        with open(f"{result['name']}_full.json", 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\n已保存完整数据到: {result['name']}_full.json")
